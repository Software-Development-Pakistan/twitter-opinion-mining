

/**
  *  Copyright 2016 Aalto University, Ahmed Hussnain
  *
  *  Licensed under the Apache License, Version 2.0 (the "License");
  *  you may not use this file except in compliance with the License.
  *  You may obtain a copy of the License at
  *
  *     http://www.apache.org/licenses/LICENSE-2.0
  *
  *  Unless required by applicable law or agreed to in writing, software
  *  distributed under the License is distributed on an "AS IS" BASIS,
  *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  *  See the License for the specific language governing permissions and
  *  limitations under the License.
  */

import scala.collection.JavaConverters._
import org.apache.spark.rdd.RDD
import org.apache.spark.{ SparkConf, SparkContext }
import org.apache.spark.sql._
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.sql.types._
import org.apache.spark.ml.feature.{CountVectorizer, RegexTokenizer, StopWordsRemover}
import org.apache.spark.ml.classification.LogisticRegression
import org.apache.spark.ml.Pipeline
import org.apache.spark.mllib.clustering.{ OnlineLDAOptimizer, LDA, LocalLDAModel }    // NB! Optimizer is exchangeable
import org.apache.spark.mllib.linalg.{ Vector, Vectors }
import org.apache.spark.mllib.rdd.RDDFunctions._
import org.apache.spark.storage.StorageLevel
import com.typesafe.config.{Config, ConfigFactory}
import org.apache.spark.SparkContext._



object TwitterOpinionMining {

  def main(args: Array[String]): Unit = {

    /* === Loading config === */
    val config: Config = ConfigFactory.load("config.json")

    /* === Setup === */
    val sparkConf = new SparkConf()
      .setAppName(config.getString("sparkSettings.appName"))
      //.set("spark.driver.memory", config.getString("sparkSettings.executorMemory"))
      //.set("spark.executor.memory",config.getString("sparkSettings.executorMemory"))
    val sCtx = new SparkContext(sparkConf)
    val sqlCtx = new HiveContext(sCtx)
    import org.apache.spark.SparkContext._
    import sqlCtx.implicits._




    /* === Miscellaneous === */
    val outputPath = config.getString("output.csvOutputPath")
    val stopwords: Array[String] = sCtx // An array of words to be ignored by all topic modelling.
      .textFile(config.getString("input.stopwords"))
      .flatMap(_.stripMargin.split("\\s+"))
      .collect ++ Array("rt")
    val tweetTimeFormat = new java.text.SimpleDateFormat("EEE MMM dd HH:mm:ss ZZZZZ yyyy") // Twitter's time format

    /* === Peak detection === */
    /**
      * Generates possible delta values for the positive and negative tweets separately.
      * Returns a tuple containing the quadratic mean of the difference between the primary filtered elements and the
      * secondary filtered elements for the positive tweets and that of the negative ones.
      */
    def deltaDev(smoothenedData: RDD[(Long, (Int, Int, Int), (Int, Int, Int))]) = {
      val sum = smoothenedData.map { case (i, (v1, s1, r1), (v2, s2, r2)) => (Math.pow(r1 - s1, 2), Math.pow(r2 - s2, 2)) }.reduce { case (a, b) => (a._1 + b._1, a._2 + b._2) }
      val n = smoothenedData.count()
      (Math.round(Math.sqrt(sum._1.toDouble / n)).toInt, Math.round(Math.sqrt(sum._2.toDouble / n)).toInt)
    }
    val ALPHA = config.getInt("peakDetection.alpha") // The roughness of the primary peak detection filter (defines moving average window size)
    val BETA = config.getInt("peakDetection.beta")// The roughness of the secondary peak detection filter (defines moving average window size). By definition, BETA > ALPHA.
    val DELTA_GEN: (RDD[(Long, (Int, Int, Int), (Int, Int, Int))]) => (Int, Int) = deltaDev // A function that generates two delta values (one for positive and one for negative tweets). A larger delta value places higher requirements on local maxima to be considered 'peaks'.

    /* === Topic modelling === */
    val LDA_i = config.getInt("topicModelling.i") // The number of iterations performed by the topic modelling
    val LDA_k = config.getInt("topicModelling.k") // The number of topics to be generated by the topic modelling
    val LDA_n = config.getInt("topicModelling.n") // The number of words generated per topic
    val tmWnd = config.getInt("topicModelling.wnd") // The topic modelling window, i.e. the number of time intervals that are to be considered part of a peak. E.g. if there is a peak at t=10, and tmWnd is 3, tweets will be analyzed from t=9 to (and including) t=11. NB! Must be an odd, positive number.
    val topicSample = config.getInt("topicModelling.topicSample") // The amount of sample tweets displayed per topic.

    /**
      * Takes as parameters a time as a string in the twitter date format (the query
      * time) and a java.util.Date (the reference time, or the "start of time") and a
      * Long which defines the length (in milliseconds) of the intervals. Returns the
      * index of the interval to which the raw date belongs.
      */
    def getOffset(rawDate: String, base: java.util.Date, interval: Long) = {
      val date = tweetTimeFormat.parse(rawDate)
      val offset = date.getTime - base.getTime
      offset / interval
    }

    /**
      * Takes a string of text as input and returns a sequence of all the hashtags
      * that were found in the text (in lower case).
      */
    def getHashTags(text: String) = {
      text.split("\\s+").filter(_.startsWith("#"))
        .map(_.tail.span(c => c.isLetterOrDigit || c == '_')._1.toLowerCase)
        .filter(tag => !tag.isEmpty && tag.head.isLetter).toVector
    }

    /**
      * Loads the data, applies sentiment analysis on each tweet and returns relevant information about them.
      * @return a DataFrame containing the prepared data, in the form [id, created_at, screen_name, text, probability, prediction]
      */
    def prepare() = {

      val df = sqlCtx.read
        .format("com.databricks.spark.csv")
        .option("header", "true") // Use first line of all files as header
        .option("inferSchema", "true") // Automatically infer data types
        .load(config.getString("input.trainingData"))

      df.createOrReplaceTempView("ttrain")

      val trainingData = sqlCtx.sql("select created_at,user_screenname,text, cast (if(label=4,1,0) as double) as label from ttrain").cache()

      val tokenizer = new RegexTokenizer()
        .setGaps(false)
        .setPattern("\\p{L}+")
        .setInputCol("text")
        .setOutputCol("words")

      val filterer = new StopWordsRemover()
        .setStopWords(stopwords)
        .setCaseSensitive(false)
        .setInputCol("words")
        .setOutputCol("filtered")

      val countVectorizer = new CountVectorizer()
        .setInputCol("filtered")
        .setOutputCol("features")

      val lr = new LogisticRegression()
        .setMaxIter(10)
        .setRegParam(0.2)
        .setElasticNetParam(0.0)

      val pipeline = new Pipeline().setStages(Array(tokenizer, filterer, countVectorizer, lr))

      val lrModel = pipeline.fit(trainingData)

      val t_df = sqlCtx.read.json(config.getString("input.tweets"))
      t_df.createOrReplaceTempView("test")

      val testData = sqlCtx.sql("select id,created_at,user.screen_name,text from test where lang='en' and text is not null ").persist(StorageLevel.MEMORY_AND_DISK_SER)
      testData.createOrReplaceTempView("testing")

      val predicted = lrModel.transform(testData).select("id", "created_at", "screen_name", "text", "probability", "prediction")

      sqlCtx.dropTempTable("ttrain")
      sqlCtx.dropTempTable("test")
      predicted
    }

    /**
      * Splits the tweets into fixed size (now hourly) time intervals and returns an list (DataFrame) of these (as
      * indices starting from 0: interval number 0 being that which starts at t0=baseTime and ends at t1=baseTime + 1h)
      * with their respective amount of positive and negative tweets.
      * @param predicted A DataFrame of schema [id, created_at, screen_name, text, probability, prediction], containing all tweets
      * @param baseTime The beginning of time (e.g. the time of the first tweet in the dataset)
      * @return A DataFrame mapping each time interval (by index) to the amount of positive and negative tweets that were created during that interval
      */
    def getHourlyTweetCount(predicted: DataFrame, baseTime: java.util.Date) = {
      val data = predicted.select("created_at", "prediction")   // RDD[(index: Long, (n_pos: Long, n_neg: Long)]
        .map(row => (row(0).toString, row(1).toString.toDouble.toInt > 0))
        .map { case (time, prediction) => (getOffset(time, baseTime, 3600000), if (prediction) (1L, 0L) else (0L, 1L)) }
        .reduceByKey { case (a, b) => (a._1 + b._1, a._2 + b._2) }
        .map { case (index, (n_pos, n_neg)) => Row(index, n_pos, n_neg) }
      val dataStruct = StructType(
          StructField("index", LongType, false) ::
          StructField("positive_tweets", LongType, false) ::
          StructField("negative_tweets", LongType, false) :: Nil)
      sqlCtx.createDataFrame(data, dataStruct)
    }

    /** The following functions are different smoothing algorithms. They are all based on a moving window,
      * but they use different mathematical means and different weightings. What they all have in common is
      * that they take one parameter: the size of the moving window, which (to a greater or lesser degree)
      * defines how heavy the filtering becomes.
      *
      * Below are three different smoothing algorithms, each consisting of three functions adapted and applied
      * to their current purpose. There is the helper function, which calculates the means and returns them,
      * as well as a function to apply the corresponding filter as the primary (light) filtering and a function
      * to do the same with the secondary (heavy) filtering.
      * The algorithms are:
      *
      * 1. Plain average: Applies a standard arithmetic average on all the elements in the window.
      * 2. Arithmetically weighted average: Applies an arithmetic average on all the elements in the window,
      * arithmetically weighted in favour of the middle value.
      * 3. Polynomially weighted average: Applies an arithmetic average on all the elements in the window,
      * polynomially (2nd degree) weighted in favour of the middle value.
      */

    /* Plain average */
    def plainAverageHelper(window: Array[(Long, (Int, Int, Int), (Int, Int, Int))]) = {
      var sum1 = 0
      var sum2 = 0
      for (j <- window.indices) {
        sum1 += window(j)._2._1
        sum2 += window(j)._3._1
      }
      (sum1, sum2)
    }
    def plainAverageLight(window: Array[(Long, (Int, Int, Int), (Int, Int, Int))]) = {
      val center = window(window.length / 2)
      val (sum1, sum2) = plainAverageHelper(window)
      (center._1, (center._2._1, scala.math.round(sum1.toFloat / window.length), center._2._3), (center._3._1, scala.math.round(sum2.toFloat / window.length), center._3._3))
    }
    def plainAverageHeavy(window: Array[(Long, (Int, Int, Int), (Int, Int, Int))]) = {
      val center = window(window.length / 2)
      val (sum1, sum2) = plainAverageHelper(window)
      (center._1, (center._2._1, center._2._2, scala.math.round(sum1.toFloat / window.length)), (center._3._1, center._3._2, scala.math.round(sum2.toFloat / window.length)))
    }

    /* Arithmetically weighted average */
    def arithmeticallyWeightedAverageHelper(window: Array[(Long, (Int, Int, Int), (Int, Int, Int))]) = {
      val wnd = window.length / 2
      var sum1 = 0.0
      var sum2 = 0.0
      for (j <- window.indices) {
        val weight = (wnd + 1 - Math.abs(wnd - j)) * 1.0 / Math.pow(wnd + 1, 2)
        sum1 += window(j)._2._1 * weight
        sum2 += window(j)._3._1 * weight
      }
      (sum1, sum2)
    }
    def arithmeticallyWeightedAverageLight(window: Array[(Long, (Int, Int, Int), (Int, Int, Int))]) = {
      require(window.length % 2 == 1)
      val center = window(window.length / 2)
      val (sum1, sum2) = arithmeticallyWeightedAverageHelper(window)
      (center._1, (center._2._1, Math.round(sum1).toInt, center._2._3), (center._3._1, Math.round(sum2).toInt, center._3._3))
    }
    def arithmeticallyWeightedAverageHeavy(window: Array[(Long, (Int, Int, Int), (Int, Int, Int))]) = {
      require(window.length % 2 == 1)
      val center = window(window.length / 2)
      val (sum1, sum2) = arithmeticallyWeightedAverageHelper(window)
      (center._1, (center._2._1, center._2._2, Math.round(sum1).toInt), (center._3._1, center._3._2, Math.round(sum2).toInt))
    }

    /* Ponynomially (2nd degree) weighted average */
    def polynomiallyWeightedAverageHelper(window: Array[(Long, (Int, Int, Int), (Int, Int, Int))]) = {
      val wnd = window.length / 2
      var sum1 = 0.0
      var sum2 = 0.0
      for (j <- window.indices) {
        val preWeight = Math.pow(wnd - Math.abs(wnd - j) + 1, 2)
        sum1 += window(j)._2._1 * preWeight
        sum2 += window(j)._3._1 * preWeight
      }
      val postWeight = 3.0 / (2 * Math.pow(wnd, 3) + 6 * Math.pow(wnd, 2) + 7 * wnd + 3)
      (Math.round(sum1 * postWeight).toInt, Math.round(sum2 * postWeight).toInt)
    }
    def polynomiallyWeightedAverageLight(window: Array[(Long, (Int, Int, Int), (Int, Int, Int))]) = {
      require(window.length % 2 == 1)
      val center = window(window.length / 2)
      val (res1, res2) = polynomiallyWeightedAverageHelper(window)
      (center._1, (center._2._1, res1, center._2._3), (center._3._1, res2, center._3._3))
    }
    def polynomiallyWeightedAverageHeavy(window: Array[(Long, (Int, Int, Int), (Int, Int, Int))]) = {
      require(window.length % 2 == 1)
      val center = window(window.length / 2)
      val (res1, res2) = polynomiallyWeightedAverageHelper(window)
      (center._1, (center._2._1, center._2._2, res1), (center._3._1, center._3._2, res2))
    }

    /**
      * Smoothens the input data according to the given parameters.
      *
      * @param data       an RDD whose elements are of the following form: (index, (pos_twts, pos_twts_filt1, pos_twts_filt2), (neg_twts, neg_twts_filt1, neg_twts_filt2), where the filtered values are given as 0
      * @param wnd        the wnd, or alpha, parameter which decides the size of the moving window and thus, in most algorithms, the heaviness of the filter. The window size equals 2*wnd+1.
      * @param smoothener the smoothening function to be applied on each window
      * @return an RDD of the same form as the input data, where pos_twts_filt1, pos_twts_filt2, neg_twts_filt1 and neg_twts_filt2 are filled in
      */
    def smoothenCustom(data: RDD[(Long, (Int, Int, Int), (Int, Int, Int))], wnd: Int, smoothener: (Array[(Long, (Int, Int, Int), (Int, Int, Int))]) => (Long, (Int, Int, Int), (Int, Int, Int))) = {
      val dataSize = data.count()
      val prefix = (-wnd until 0).map(i => (i.toLong, (0, 0, 0), (0, 0, 0)))
      val suffix = (dataSize until dataSize + wnd).map(i => (i, (0, 0, 0), (0, 0, 0)))
      val preparedData = sCtx.parallelize(prefix) ++ data ++ sCtx.parallelize(suffix)
      preparedData.sliding(wnd * 2 + 1)
        .map(window => smoothener(window))
    }

    /**
      * Marks out those elements which are considered 'peaks of interest' by the peak detection algorithm.
      *
      * @param data an RDD of the form: (index, (pos_twts, pos_twts_filt1, pos_twts_filt2), (neg_twts, neg_twts_filt1, neg_twts_filt2), containing information about tweets per hour and their filtered values, for positive and negative tweets separately
      * @param deltas a tuple (delta1, delta2) containing the delta values for the positive and negative tweets respectively. A higher delta value places stronger requirements on which peaks that are considered 'peaks of interest'
      * @return the input data, mapped to contain two boolean values for each element: One indicating whether the element is a peak for positive tweets and one indicating wheter it is a peak for negative tweets
      */
    def markPeaks(data: RDD[(Long, (Int, Int, Int), (Int, Int, Int))], deltas: (Int, Int)): RDD[(Long, (Int, Int, Int, Boolean), (Int, Int, Int, Boolean))] = {

      /**
        * Decides whether the middle value of a triple is to be considered a 'peak of interest'.
        * The triple is given as three triples of the form (data at point (d_i), primary filtered data at point (p_i), secondary
        * filtered data at point (s_i)).
        * @param a A triple representing a point on the graph (d_i, p_i, s_i), adjacent to the left of point b
        * @param b A triple representing a point on the graph (d_i, p_i, s_i), between points a and c
        * @param c A triple representing a point on the graph (d_i, p_i, s_i), adjacent to the right of point c
        * @param delta The delta parameter
        * @return true, if b is to be considered a 'peak of interest', false otherwise.
        */
      def isPeak(a: (Int, Int, Int), b: (Int, Int, Int), c: (Int, Int, Int), delta: Int) = {
          a._2 < b._2 &&
          c._2 <= b._2 && // Simplification (May cause unbalance in the case of completely flat plateaus. Assuming that these are very rare, the simplification should have little impact on the result.)
          b._2 > b._3 + delta
      }

      val prefix = Vector((-1L, (0, 0, 0), (0, 0, 0)))
      val suffix = Vector((data.count(), (0, 0, 0), (0, 0, 0)))
      val preparedData = sCtx.parallelize(prefix) ++ data ++ sCtx.parallelize(suffix)
      preparedData.sliding(3)
        .map(triple => (
          triple(1)._1,
          (triple(1)._2._1, triple(1)._2._2, triple(1)._2._3, isPeak(triple(0)._2, triple(1)._2, triple(2)._2, deltas._1)),
          (triple(1)._3._1, triple(1)._3._2, triple(1)._3._3, isPeak(triple(0)._3, triple(1)._3, triple(2)._3, deltas._2))
          ))
    }

    /**
      * Applies smoothening and peak detection on the input data based on the parameters.
      *
      * The algorithm is as follows:
      *      1. Primary and secondary filters are separately applied on the data.
      *      2. All absolute peaks (local maxima; and possibly the end points) are found.
      *      3. The absolute peaks are compared to the corresponding position in the secondary filtered data to
      * determine if they are pronounced enough and thus considered peaks of interest. All such elements are marked as
      * peaks.
      *
      * Note that the secondary filtering must (by definition) be heavier than the primary filtering.
      *
      * @param data            the RDD containing the data (as amount of positive and negative tweets in the specified unit of time)
      * @param alpha           defines the heaviness of the primary smoothening filter. 2*alpha+1 is the size of the smoothening window
      * @param beta            defines the heaviness of the secondary smoothening filter. 2*beta+1 is the size of the smoothening window
      * @param deltaGen        a function that returns the peak thresholds for positive and negative tweets (i.e. two the delta values)
      * @param smoothenerLight the smoothening function that will be applied on each window in the primary filtering
      * @param smoothenerHeavy the smoothening function that will be applied on each window in the secondary filtering
      * @return the input data mapped to also contain both filters and a boolean indicating whether the element is a peak
      */
    def peakProcess(data: RDD[(Long, (Int, Int))], alpha: Int, beta: Int,
                    deltaGen: (RDD[(Long, (Int, Int, Int), (Int, Int, Int))]) => (Int, Int),
                    smoothenerLight: (Array[(Long, (Int, Int, Int), (Int, Int, Int))]) => (Long, (Int, Int, Int), (Int, Int, Int)),
                    smoothenerHeavy: (Array[(Long, (Int, Int, Int), (Int, Int, Int))]) => (Long, (Int, Int, Int), (Int, Int, Int))) = {
      val maxIndex = data.reduce { case (a, b) => if (a._1 > b._1) a else b }._1
      val shadow = sCtx.parallelize(0L to maxIndex).map((_, (0, 0)))
      val preprocessedData = data
        .++(shadow)
        .reduceByKey { case (a, b) => (a._1 + b._1, a._2 + b._2) }
        .map { case (i, (vPos, vNeg)) => (i, (vPos, 0, 0), (vNeg, 0, 0)) }
        .sortBy(_._1)
      val smoothenedData = smoothenCustom(smoothenCustom(preprocessedData, alpha, smoothenerLight), beta, smoothenerHeavy)
      val (delta1, delta2) = deltaGen(smoothenedData)
      val peakMarkedData = markPeaks(smoothenedData, (delta1, delta2))
      peakMarkedData
    }

    /**
      * A DataFrame containing every tweet that is considered to be within a peak
      * (according to the peak detection algorithm and the topic modelling window)
      * together with information about during which hour it was created, which
      * hour is considered the peak hour of the peak and whether the tweet is
      * considered positive or negative.
      * @param predicted A DataFrame of schema [id, created_at, screen_name, text, probability, prediction], containing all tweets
      * @param baseTime The beginning of time (e.g. the time of the first tweet in the dataset)
      * @return A DataFrame containing all the intervals belonging to a peak (tmWnd/2 before and tmWnd/2 after the peak hour itself) and their data, in the form: [index, peak_index, positive, text]
      */
    def getPeakTweets(predicted: DataFrame, baseTime: java.util.Date): DataFrame = {

      val hourlyTweets = predicted.select("created_at", "prediction") // RDD[(index: Long, (n_pos: Int, n_neg: Int)]
        .map(row => (row(0).toString, row(1).toString.toDouble.toInt > 0))
        .map { case (time, prediction) => (getOffset(time, baseTime, 3600000), if (prediction) (1, 0) else (0, 1)) }
        .reduceByKey { case (a, b) => (a._1 + b._1, a._2 + b._2) }

      val peakMarkedDataStruct = StructType(
          StructField("index", LongType, false) ::
          StructField("positive_tweets", IntegerType, false) ::
          StructField("positive_smoothed", IntegerType, false) ::
          StructField("positive_rough", IntegerType, false) ::
          StructField("positive_peak", BooleanType, false) ::
          StructField("negative_tweets", IntegerType, false) ::
          StructField("negative_smoothed", IntegerType, false) ::
          StructField("negative_rough", IntegerType, false) ::
          StructField("negative_peak", BooleanType, false) :: Nil)

      val peakProcessed = peakProcess(hourlyTweets, ALPHA, BETA, DELTA_GEN, polynomiallyWeightedAverageLight, polynomiallyWeightedAverageHeavy)
      val peakProcessed_as_rows = peakProcessed.map { case (i, (v1, s1, r1, p1), (v2, s2, r2, p2)) => Row(i, v1, s1, r1, p1, v2, s2, r2, p2) }
      val peakProcessed_as_dataFrame = sqlCtx.createDataFrame(peakProcessed_as_rows, peakMarkedDataStruct)

      // All hours considered to be within a peak. Positive and negative peaks separately (can contain duplicate hours for this reason):
      val peaks = {
        val peakProcessedWindowed = peakProcessed_as_dataFrame
          .select("index", "positive_peak", "negative_peak")
          .rdd
          .sliding(tmWnd)
          .filter(q => q(tmWnd / 2).getBoolean(1) || q(tmWnd / 2).getBoolean(2))
          .flatMap(q => {
            val middle = q(tmWnd / 2)
            q.map(row => Row(row.getLong(0), middle.getLong(0), middle.getBoolean(1), middle.getBoolean(2)))
          })

        val peakProcessedWindowedDataStruct = StructType(
            StructField("interval_index", LongType, false) ::
            StructField("peak_index", LongType, false) ::
            StructField("positive_peak", BooleanType, false) ::
            StructField("negative_peak", BooleanType, false) :: Nil)

        val peakProcessedWindowedDataFrame = sqlCtx.createDataFrame(peakProcessedWindowed, peakProcessedWindowedDataStruct)

        peakProcessedWindowedDataFrame.createOrReplaceTempView("peakProcessedWindowed")

        sqlCtx.sql("select interval_index, peak_index, true as peak_positive from peakProcessedWindowed where positive_peak union select interval_index, peak_index, false as peak_positive from peakProcessedWindowed where negative_peak order by interval_index")
      }

      // Every tweet: its text, during which hour it was created and whether it is considered positive or negative.
      val tweets = {
        val data = predicted.select("created_at", "prediction", "text")
          .map { case Row(time, prediction, text) =>
            val offset = getOffset(time.toString, baseTime, 60 * 60 * 1000)
            Row(offset, prediction.toString.toDouble == 1, text)
          }

        val dataStruct = StructType(
            StructField("tweet_index", LongType, false) ::
            StructField("tweet_positive", BooleanType, false) ::
            StructField("text", StringType, false) :: Nil)

        sqlCtx.createDataFrame(data, dataStruct)
      }

      tweets.join(peaks, tweets("tweet_index") === peaks("interval_index") && tweets("tweet_positive") === peaks("peak_positive"), "inner")
        .createOrReplaceTempView("peakTweets")
      val result = sqlCtx.sql("select interval_index as index, peak_index, peak_positive as positive, text from peakTweets order by index")
      sqlCtx.dropTempTable("peakProcessedWindowed")
      sqlCtx.dropTempTable("peakTweets")
      result
    }

    /**
      * Performs the necessary preprocessing by turning the data into the form required by LDA topic modelling.
      * @param tweets An RDD of the tweet texts that are to be processed together
      * @param stopwords An Array of stopwords, i.e. words not to be considered by the topic modelling
      * @return A tuple (documents, vocabulary), where documents is an RDD of vectorized documents and vocabulary is an array of all the (unskipped) words that appeared in the tweets, sorted by frequency.
      */
    def ldaPreprocess(tweets: RDD[String], stopwords: Array[String]): (RDD[(Long, Vector)], Array[String]) = {

      /* Turns a tweet into an array of its words, excluding stopwords and words that are not considered proper words. */
      def tokenize(tweet: String) = tweet
        .split("\\s")
        .filter(word => !word.startsWith("http://") && !word.startsWith("https://") && !word.startsWith("www."))    // Temporary simple implementation to avoid most links. TODO: Replace with a regex that identifies all unwanted strings.
        .map(_.takeWhile(_.isLetterOrDigit).toLowerCase)            // "What?" -> "what", "Microsoft's" -> "microsoft", "OS2016" -> "os2016"
        //.map(_.filter(_.isLetterOrDigit).toLowerCase)             // "What?" -> "what", "Microsoft's" -> "microsofts", "OS2016" -> "os2016"
        //.map(word => if (!word.isEmpty && !word.last.isLetter) word.dropRight(1).toLowerCase else word.toLowerCase)     // "What?" -> "what", "Microsoft's" -> [filtered], "OS2016" -> [filtered]
        .filter(word => word.length > 2 && word.forall(_.isLetterOrDigit) && !stopwords.contains(word))

      /* An RDD of the tokenized tweets. */
      val tokens: RDD[Array[String]] = tweets.map(tokenize)

      /* An array of all the (unskipped) words that appeared in the tweets, sorted by frequency. */
      val vocabulary: Array[String] = tokens
        .flatMap(arr => arr)
        .map((_, 1L))
        .reduceByKey(_ + _)
        .collect
        .sortBy(-_._2)
        .map(_._1)

      /* A Map where the words in the vocabulary are mapped to their corresponding indices. */
      val vocabularyMap: Map[String, Int] = vocabulary
        .zipWithIndex
        .toMap

      /* An RDD of the vectorized documents. */
      val documents: RDD[(Long, Vector)] = tokens
        .zipWithIndex
        .map { case (tweet, i) =>
          val frequencies = tweet
            .distinct
            .toSeq
            .filter(word => vocabulary.contains(word))
            .map(word => (vocabularyMap(word), tweet.count(_ == word).toDouble))
          (i, Vectors.sparse(vocabulary.length, frequencies))
        }

      (documents, vocabulary)
    }

    /**
      * Applies LDA topic modelling to the tweets.
      * @param documents An RDD of vectorized documents
      * @return The resulting LDAModel
      */
    def lda(documents: RDD[(Long, Vector)]) = {
      val lda = new LDA()
        .setOptimizer(new OnlineLDAOptimizer())
        .setK(LDA_k)
        .setMaxIterations(LDA_i)
      lda.run(documents)
    }

    /**
      * Applies topic modelling to and extracts hashtags from the tweets belonging to each peak separately and creates
      * a list of relevant information for each time interval.
      * @param peakTweets A DataFrame containing all the intervals belonging to a peak (tmWnd/2 before and tmWnd/2 after the peak hour itself) and their data, in the form: [index, peak_index, positive, text]
      * @param hourlyTweetCount A DataFrame mapping each time interval (by index) to the amount of positive and negative tweets that were created during that interval
      * @param baseTime The beginning of time (e.g. the time of the first tweet in the dataset)
      * @return A DataFrame of each time interval (separate entries for positive and negative tweets). It has the following schema: [hour, sentiment, tweets, topics, hashtags], where 'topics' and 'hashtags' are empty for time intervals that are not peaks (those that are, include topics and hashtags from the previous tmWnd/2 and the next tmWnd/2 intervals as well)
      */
    def process(peakTweets: DataFrame, hourlyTweetCount: DataFrame, baseTime: java.util.Date) = {
      val baseTimeMillis = baseTime.getTime

      val peakTweetTexts: Array[((Long, Boolean), RDD[String])] = {
        val peaks = peakTweets
          .map(row => ((row.getLong(1), row.getBoolean(2)), row.getString(3)))
        val peakIndices = peaks
          .map(_._1)
          .distinct
          .collect
        peakIndices.map(k => (k, peaks.filter(_._1 == k).map(_._2)))
      }

      val peakTweetTextsAndHashtags = peakTweetTexts
        .map { case (key, tweets) => (key, tweets, tweets.flatMap(getHashTags)
                                                          .map(tag => (tag, 1L))
                                                          .reduceByKey(_ + _)
                                                          .takeOrdered(10)(new Ordering[(String, Long)]() {
                                                            override def compare(x: (String, Long), y: (String, Long)): Int = {
                                                              (y._2 - x._2).toInt
                                                            }
                                                          }).map(_._1)
          ) }

      /* Each individual peak (positive and negative ones separately) with the topics generated from their tweet texts: */
      val topicModelledPeaks = {
        val data = peakTweetTextsAndHashtags.map { case (key, tweets_raw, hashtags) =>
          val tweets = tweets_raw.map(_.map(c => if (c == '\n') ' ' else if (c == '|') '\\' else c))
          val (documents, vocabulary) = ldaPreprocess(tweets, stopwords)
          val index: Long = key._1
          val positive: Boolean = key._2
          val ldaModel = lda(documents)
          val topics: String = ldaModel
            .describeTopics(LDA_n)
            .map(_._1.map(i => vocabulary(i)).mkString(","))
            .mkString("; ")
          val topicDistributions = ldaModel.asInstanceOf[LocalLDAModel].topicDistributions(documents)
          val sampleTweets = (0 until LDA_k).flatMap(topicIndex => {
            val distr = topicDistributions.takeOrdered(topicSample)(new Ordering[(Long, Vector)]() {
              override def compare(x: (Long, Vector), y: (Long, Vector)): Int = {
                if (x._2(topicIndex) < y._2(topicIndex)) 1 else -1
              }
            })
            val tweetsByIndex = tweets.zipWithIndex.map(_.swap)
            distr.map(d => tweetsByIndex.lookup(d._1).head)
          }).mkString("|")
          val formattedHashtags = hashtags.mkString(",")
          Row(index, positive, topics, sampleTweets, formattedHashtags)
        }
        val dataStruct = StructType(
            StructField("index", LongType, false) ::
            StructField("positive", BooleanType, false) ::
            StructField("topics", StringType, false) ::
            StructField("sampleTweets", StringType, false) ::
            StructField("hashtags", StringType, false) :: Nil)
        sqlCtx.createDataFrame(data.toList.asJava, dataStruct)
      }

      val tmData = {
        topicModelledPeaks.createOrReplaceTempView("topicModelledPeaks")
        hourlyTweetCount.createOrReplaceTempView("baseHourlyTweets")
        sqlCtx.sql("select index, true as positive, positive_tweets as tweets from baseHourlyTweets union select index, false as positive, negative_tweets as tweets from baseHourlyTweets").createOrReplaceTempView("baseHourlySeparatedTweets")
        val indexedRes = sqlCtx.sql("select baseHourlySeparatedTweets.index, baseHourlySeparatedTweets.positive as sentiment, tweets, topics, sampleTweets, hashtags from baseHourlySeparatedTweets full outer join topicModelledPeaks on baseHourlySeparatedTweets.index=topicModelledPeaks.index and baseHourlySeparatedTweets.positive=topicModelledPeaks.positive order by index, sentiment")

        val data = indexedRes.map { case Row(i: Long, s, n, t, st, h) => Row(new java.util.Date(baseTimeMillis + i * 3600000).toString, s, n, t, st, h) }
        val dataStruct = StructType(
            StructField("hour", StringType, false) ::
            StructField("sentiment", BooleanType, false) ::
            StructField("tweets", LongType, false) ::
            StructField("topics", StringType, false) ::
            StructField("sampleTweets", StringType, false) ::
            StructField("hashtags", StringType) :: Nil)
        sqlCtx.createDataFrame(data, dataStruct)
      }

      sqlCtx.dropTempTable("topicModelledPeaks")
      sqlCtx.dropTempTable("baseHourlyTweets")
      sqlCtx.dropTempTable("baseHourlySeparatedTweets")
      tmData
    }



    // Execution starts here:
    println("Starting. Finding predicted:")
    val predicted = prepare()
    println("Successful. Finding baseTime:")
    val baseTime = tweetTimeFormat.parse(
      predicted.select("created_at")
        .rdd
        .map(_.getString(0))
        .min()(new Ordering[String]() {
          override def compare(x: String, y: String): Int =
            tweetTimeFormat.parse(x).compareTo(tweetTimeFormat.parse(y))
        })
    )
    println("Successful! Finding hourlyTweetCount:")
    val hourlyTweetCount = getHourlyTweetCount(predicted, baseTime)
    println("Successful! Finding peakTweets:")
    val peakTweets = getPeakTweets(predicted, baseTime)
    println("Successful! Finding tmData:")
    val tmData = process(peakTweets, hourlyTweetCount, baseTime)
    println(tmData.show())
    println("Successful. Coalescing to one:")
    val toWrite = tmData.coalesce(1)
    println("Successful.")

    // Writing to file starts here:
    println("Attempting to write to file:")
    toWrite.write
      .mode("overwrite")
      .format("com.databricks.spark.csv")
      .option("header", "true")
      .save(outputPath)
    println("Successful.")
    // Writing to file ends here

    // Writing to database starts here:
    /*println("Attempting to save to database:")
    val properties = new java.util.Properties()
    properties.setProperty("database", config.getString("output.database"))
    properties.setProperty("user", config.getString("output.user"))
    properties.setProperty("password", config.getString("output.password"))
    properties.setProperty("encrypt", "true")
    properties.setProperty("trustServerCertificate", "false")
    properties.setProperty("hostNameInCertificate", "*.database.windows.net")
    properties.setProperty("loginTimeout", "30")
    toWrite.write
      .mode("append")
      .jdbc(s"jdbc:sqlserver://${config.getString("output.hostname")}", config.getString("output.table"),properties)
    // Writing to database ends here

    println("Successful? Ending program.")
    // Execution ends here*/

  }
}
